{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Visualization_Q6.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing the required packages\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import cv2"
      ],
      "metadata": {
        "id": "cEozejXIZt12"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb --upgrade\n",
        "import wandb\n",
        "api = wandb.Api()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "FWSS3WDQZxRk",
        "outputId": "74c1070e-c62a-4c69-8808-33827eff8e6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.16-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 31.1 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |▌                               | 30 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |▊                               | 40 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 61 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 71 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 92 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 102 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 112 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 122 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 133 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 143 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 153 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 163 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 174 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 184 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 194 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 204 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 215 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████                            | 225 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 235 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 245 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 256 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 266 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████                           | 276 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 286 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 296 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 307 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 317 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 327 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 337 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 348 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 358 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 368 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 378 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 389 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 399 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 409 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 419 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 430 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 440 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 450 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 460 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 471 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 481 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 491 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 501 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 512 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 522 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 532 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 542 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 552 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 563 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 573 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 583 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 593 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 604 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 614 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 624 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 634 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 645 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 655 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 665 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 675 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 686 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 696 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 706 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 716 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 727 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 737 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 747 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 757 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 768 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 778 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 788 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 798 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 808 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 819 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 829 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 839 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 849 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 860 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 870 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 880 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 890 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 901 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 911 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 921 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 931 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 942 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 952 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 962 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 972 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 983 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 993 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.0 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.0 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.0 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.0 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.0 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.1 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.1 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.1 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.1 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.1 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.1 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.1 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.1 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.2 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.2 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.2 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.2 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.2 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.2 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.2 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.3 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.3 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.3 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.3 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.3 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.3 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.4 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.4 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.4 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.4 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.4 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.4 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.4 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.4 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.4 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.4 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.5 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.5 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.5 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.5 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.5 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.5 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.5 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.5 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.5 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.5 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.6 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.6 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.6 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.6 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.6 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.6 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.6 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.6 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.7 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.7 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.7 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.7 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.7 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.7 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.7 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.7 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.8 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.8 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.8 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8 MB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.8 MB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.11-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 68.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 68.7 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=3cfba34fbfc4afbd83c3848ea1d3d655d7f4daef73e0caecbeab5decf4db2cad\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.11 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XsecTVDsXZMg"
      },
      "outputs": [],
      "source": [
        "# b9780837d34fae7e3219f11d7f38fe86fc87d616\n",
        "%%capture\n",
        "!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
        "!tar -xf dakshina_dataset_v1.0.tar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/dakshina_dataset_v1.0/hi/lexicons/* ./"
      ],
      "metadata": {
        "id": "HpUkOim-eBWg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"hi.translit.sampled.train.tsv\"\n",
        "dev_path = \"hi.translit.sampled.dev.tsv\"\n",
        "test_path = \"hi.translit.sampled.test.tsv\""
      ],
      "metadata": {
        "id": "BP21v9RxeDpR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
      ],
      "metadata": {
        "id": "M_Y5v_bteGZz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "def load_data(path):\n",
        "    with open(path) as f:\n",
        "        data = pd.read_csv(f, sep='\\t',header=None,names=[\"indic\",\"english\",\"\"],skip_blank_lines=True,index_col=None)\n",
        "    data = data[data['indic'].notna()]\n",
        "    data = data[data['english'].notna()]\n",
        "    data = data[['indic','english']]\n",
        "    return data\n",
        "\n",
        "def preprocess(train_path, dev_path, test_path, batch_size):\n",
        "\n",
        "    train_df = load_data(train_path)\n",
        "    val_df = load_data(dev_path)\n",
        "    test_df = load_data(test_path)\n",
        "\n",
        "    train_indic = train_df['indic'].values\n",
        "    train_english = train_df['english'].values\n",
        "    val_indic = val_df['indic'].values\n",
        "    val_english = val_df['english'].values\n",
        "    test_indic = test_df['indic'].values\n",
        "    test_english = test_df['english'].values\n",
        "\n",
        "\n",
        "    # \"\\t\" is considered as the \"start\" character\n",
        "    # \"\\n\" is considered as the \"end\" character.\n",
        "\n",
        "    #We add the above characters to the indic transliterated words.\n",
        "    train_indic =  \"\\t\" + train_indic + \"\\n\"\n",
        "    val_indic =  \"\\t\" + val_indic + \"\\n\"\n",
        "    test_indic =  \"\\t\" + test_indic + \"\\n\"\n",
        "\n",
        "\n",
        "    #Create character sets for each language\n",
        "    indic_char_set = set()\n",
        "    english_char_set = set()\n",
        "\n",
        "    indic_char_set.add(' ')\n",
        "    english_char_set.add(' ')\n",
        "    \n",
        "    for word_english, word_indic in zip(train_english, train_indic):\n",
        "        for char in word_english:\n",
        "            english_char_set.add(char)\n",
        "        for char in word_indic:\n",
        "            indic_char_set.add(char)\n",
        "\n",
        "    english_char_set = sorted(list(english_char_set))\n",
        "    indic_char_set = sorted(list(indic_char_set))\n",
        "\n",
        "\n",
        "    #Create empty dicts.\n",
        "    english_char_to_idx = dict()\n",
        "    indic_char_to_idx = dict()\n",
        "\n",
        "    english_idx_to_char = dict()\n",
        "    indic_idx_to_char = dict()\n",
        "\n",
        "    #As our character sets don't consider spaces, we assign a special id -1 to space.\n",
        "    # We will pad the strings with spaces to make them of equal length, to support batchwise training.\n",
        "\n",
        "    english_char_to_idx[\" \"] = -1\n",
        "    indic_char_to_idx[\" \"] = -1\n",
        "\n",
        "    #Create a mapping of characters to indices    \n",
        "    for i, char in enumerate(english_char_set):\n",
        "        english_char_to_idx[char] = i\n",
        "\n",
        "    for i, char in enumerate(indic_char_set):\n",
        "        indic_char_to_idx[char] = i\n",
        "\n",
        "\n",
        "    #Create a mapping of indices to characters.\n",
        "\n",
        "    for char, idx in english_char_to_idx.items():\n",
        "        english_idx_to_char[idx] = char\n",
        "\n",
        "    for char, idx in indic_char_to_idx.items():\n",
        "        indic_idx_to_char[idx] = char\n",
        "    \n",
        "    #Find the max word length in the indic and english sentences respectively.\n",
        "\n",
        "    max_seq_len_english_encoder = max([len(word) for word in train_english])\n",
        "    max_seq_len_indic_decoder = max([len(word) for word in train_indic])\n",
        "\n",
        "    encoder_train_english = np.zeros((len(train_english), max_seq_len_english_encoder), dtype=\"float32\")\n",
        "    decoder_train_english = np.zeros((len(train_english), max_seq_len_indic_decoder), dtype=\"float32\")\n",
        "    decoder_train_indic = np.zeros(\n",
        "        (len(train_english), max_seq_len_indic_decoder, len(indic_char_set)), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    encoder_val_english = np.zeros(\n",
        "        (len(val_english), max_seq_len_english_encoder), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_val_english = np.zeros(\n",
        "        (len(val_english), max_seq_len_indic_decoder), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_val_indic = np.zeros(\n",
        "        (len(val_english), max_seq_len_indic_decoder, len(indic_char_set)), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    encoder_test_english = np.zeros(\n",
        "        (len(test_english), max_seq_len_english_encoder), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_test_english = np.zeros(\n",
        "        (len(test_english), max_seq_len_indic_decoder), dtype=\"float32\"\n",
        "    )\n",
        "    decoder_test_indic = np.zeros(\n",
        "        (len(test_english), max_seq_len_indic_decoder, len(indic_char_set)), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # print(encoder_train_english.shape, \"ENC Train Eng\")\n",
        "    # print(decoder_train_english.shape, \"DEC Train Eng\")\n",
        "    # print(decoder_train_indic.shape, \"DEC Train Indic\")\n",
        "    # print(encoder_val_english.shape, \"ENC Val Eng\")\n",
        "    # print(decoder_val_english.shape, \"DEC Val Eng\")\n",
        "    # print(decoder_val_indic.shape, \"DEC Val Eng\")\n",
        "    # print(encoder_test_english.shape, \"ENC Test Eng\")\n",
        "    # print(decoder_test_english.shape, \"DEC Test Eng\")\n",
        "    # print(decoder_test_indic.shape, \"DEC Test Eng\")\n",
        "  \n",
        "\n",
        "    for i, (input_word, target_word) in enumerate(zip(train_english, train_indic)):\n",
        "        for t, char in enumerate(input_word):\n",
        "            #Replace character by its index.\n",
        "            encoder_train_english[i, t] = english_char_to_idx[char]\n",
        "        #Padding with zeros.\n",
        "        encoder_train_english[i, t + 1 :] = english_char_to_idx[' ']\n",
        "        \n",
        "        for t, char in enumerate(target_word):\n",
        "            decoder_train_english[i, t] = indic_char_to_idx[char]\n",
        "            if t > 0:\n",
        "                # Indic decoder will be ahead by one timestep.\n",
        "                decoder_train_indic[i, t - 1, indic_char_to_idx[char]] = 1.0\n",
        "        #Padding with spaces.\n",
        "        decoder_train_english[i, t + 1 :] = indic_char_to_idx[' ']\n",
        "        decoder_train_indic[i, t :, indic_char_to_idx[' ']] = 1.0\n",
        "\n",
        "\n",
        "    for i, (input_word, target_word) in enumerate(zip(val_english, val_indic)):\n",
        "        for t, char in enumerate(input_word):\n",
        "            #Replace character by its index.\n",
        "            encoder_val_english[i, t] = english_char_to_idx[char]\n",
        "        #Padding with zeros.\n",
        "        encoder_val_english[i, t + 1 :] = english_char_to_idx[' ']\n",
        "        \n",
        "        for t, char in enumerate(target_word):\n",
        "            decoder_val_english[i, t] = indic_char_to_idx[char]\n",
        "            if t > 0:\n",
        "                # Indic decoder will be ahead by one timestep.\n",
        "                decoder_val_indic[i, t - 1, indic_char_to_idx[char]] = 1.0\n",
        "        #Padding with spaces.\n",
        "        decoder_val_english[i, t + 1 :] = indic_char_to_idx[' ']\n",
        "        decoder_val_indic[i, t :, indic_char_to_idx[' ']] = 1.0\n",
        "\n",
        "    for i, (input_word, target_word) in enumerate(zip(test_english, test_indic)):\n",
        "        for t, char in enumerate(input_word):\n",
        "            #Replace character by its index.\n",
        "            encoder_test_english[i, t] = english_char_to_idx[char]\n",
        "        #Padding with spaces.\n",
        "        encoder_test_english[i, t + 1 :] = english_char_to_idx[' ']\n",
        "        \n",
        "        for t, char in enumerate(target_word):\n",
        "            decoder_test_english[i, t] = indic_char_to_idx[char]\n",
        "            if t > 0:\n",
        "                # Indic decoder will be ahead by one timestep.\n",
        "                decoder_test_indic[i, t - 1, indic_char_to_idx[char]] = 1.0\n",
        "        #Padding with spaces.\n",
        "        decoder_test_english[i, t + 1 :] = indic_char_to_idx[' ']\n",
        "        decoder_test_indic[i, t :, indic_char_to_idx[' ']] = 1.0\n",
        "\n",
        "\n",
        "    return (encoder_train_english, decoder_train_english, decoder_train_indic), (encoder_val_english, decoder_val_english, decoder_val_indic), (val_english, val_indic), (encoder_test_english, decoder_test_english, decoder_test_indic), (english_char_set, indic_char_set, max_seq_len_english_encoder, max_seq_len_indic_decoder), (indic_char_to_idx, indic_idx_to_char), (english_char_to_idx, english_idx_to_char)\n",
        "    \n",
        "\n",
        "#Reference : Keras Documentation.\n",
        "#https://keras.io/examples/nlp/lstm_seq2seq/\n",
        "#https://stackoverflow.com/questions/54176051/invalidargumenterror-indicesi-0-x-is-not-in-0-x-in-keras"
      ],
      "metadata": {
        "id": "UGkPdzGReRW1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import Dense, Input,LSTM,SimpleRNN,GRU,TimeDistributed,Embedding\n",
        "from tensorflow.keras.optimizers import Adam,Nadam\n",
        "# import wandb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Concatenate, AdditiveAttention\n",
        "# from wandb.keras import WandbCallback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class Model(object):\n",
        "    def __init__(self, english_char_set, indic_char_set, max_seq_len_english_encoder, max_seq_len_indic_decoder, indic_char_to_idx, indic_idx_to_char, english_char_to_idx, english_idx_to_char, cell =\"LSTM\", optimizer = \"adam\", embedding_size = 32, num_enc_layers = 5, num_dec_layers =2, num_hidden_layers = 64, dropout = 0):\n",
        "        self.len_enc_charset = len(english_char_set)\n",
        "        self.len_dec_charset = len(indic_char_set)\n",
        "        self.max_seq_len_english_encoder = max_seq_len_english_encoder\n",
        "        self.max_seq_len_indic_decoder = max_seq_len_indic_decoder\n",
        "        self.indic_char_to_idx = indic_char_to_idx\n",
        "        self.indic_idx_to_char = indic_idx_to_char\n",
        "        self.english_char_to_idx = english_char_to_idx\n",
        "        self.english_idx_to_char = english_idx_to_char\n",
        "        self.cell = cell\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_enc_layers = num_enc_layers\n",
        "        self.num_dec_layers= num_dec_layers\n",
        "        self.num_hidden_layers =num_hidden_layers\n",
        "        self.encoder_model = None\n",
        "        self.decoder_model = None\n",
        "        self.model = None\n",
        "        self.dropout = dropout\n",
        "        self.num_epochs = None\n",
        "        self.batch_size = None\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "\n",
        "    def build_model(self):\n",
        "        encoder_inputs = Input(shape=(None,), name=\"encoder_input\")\n",
        "        encoder_outputs = Embedding(self.len_enc_charset, self.embedding_size, name = \"encoder_embedding\")(encoder_inputs)\n",
        "        self.enc_layers = []\n",
        "        self.dec_layers = []\n",
        "        encoder_states = list()\n",
        "        for j in range(self.num_enc_layers):\n",
        "            if self.cell == \"rnn\":\n",
        "                encoder = SimpleRNN(self.num_hidden_layers, dropout = self.dropout, return_state = True, return_sequences = True)\n",
        "                encoder_outputs, state = encoder(encoder_outputs)\n",
        "                encoder_states.append([state])\n",
        "                self.enc_layers.append(encoder)\n",
        "            if self.cell == \"lstm\":\n",
        "                encoder = LSTM(self.num_hidden_layers, dropout = self.dropout, return_state = True, return_sequences = True)\n",
        "                encoder_outputs, state_h, state_c = encoder(encoder_outputs)\n",
        "                encoder_states.append([state_h,state_c])\n",
        "                self.enc_layers.append(encoder)\n",
        "            if self.cell == \"gru\":\n",
        "                encoder = GRU(self.num_hidden_layers, dropout = self.dropout, return_state = True, return_sequences = True)\n",
        "                encoder_outputs, state = encoder(encoder_outputs)\n",
        "                encoder_states.append([state])\n",
        "                self.enc_layers.append(encoder)\n",
        "\n",
        "        self.encoder_model = keras.Model(encoder_inputs,encoder_states)\n",
        "\n",
        "        decoder_inputs = keras.Input(shape=(self.max_seq_len_indic_decoder, ), name = \"decoder_input\")\n",
        "      \n",
        "        decoder_outputs = Embedding(self.len_dec_charset, self.embedding_size, name = \"decoder_embedding\")(decoder_inputs)\n",
        "        decoder_states = list()\n",
        "\n",
        "        for j in range(self.num_dec_layers):\n",
        "            if self.cell == \"rnn\":\n",
        "                decoder = SimpleRNN(self.num_hidden_layers, dropout = self.dropout, return_sequences = True, return_state = True)\n",
        "                decoder_outputs, state = decoder(decoder_outputs, initial_state = encoder_states[j])\n",
        "                decoder_states.append([state])\n",
        "                self.dec_layers.append(decoder)\n",
        "            if self.cell == \"lstm\":\n",
        "                decoder = LSTM(self.num_hidden_layers, dropout = self.dropout, return_sequences = True, return_state = True)\n",
        "                decoder_outputs, state_h, state_c = decoder(decoder_outputs, initial_state = encoder_states[j])\n",
        "                decoder_states.append([state_h, state_c])\n",
        "                self.dec_layers.append(decoder)\n",
        "            if self.cell == \"gru\":\n",
        "                decoder = GRU(self.num_hidden_layers, dropout = self.dropout, return_sequences = True, return_state = True)\n",
        "                decoder_outputs, state = decoder(decoder_outputs, initial_state = encoder_states[j])\n",
        "                decoder_states.append([state])\n",
        "                self.dec_layers.append(decoder)\n",
        "\n",
        "        decoder_attn = AdditiveAttention(name=\"attention_layer\")\n",
        "        decoder_concat = Concatenate(name=\"concatenate_layer\")\n",
        "        cont_vec, attn_wts = decoder_attn([decoder_outputs, encoder_outputs],return_attention_scores=True)\n",
        "        decoder_outputs = decoder_concat([decoder_outputs,cont_vec])\n",
        "        \n",
        "        dec_dense =Dense(self.len_dec_charset, activation=\"softmax\", name=\"dense_layer\")\n",
        "        dec_pred = dec_dense(decoder_outputs)\n",
        "            \n",
        "        \n",
        "        model = keras.Model([encoder_inputs, decoder_inputs], dec_pred)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=self.optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "\n",
        "    def train(self, encoder_train_english, decoder_train_english, decoder_train_indic, encoder_val_english, decoder_val_english, decoder_val_indic, num_epochs =10, batch_size = 64):\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.model.fit(\n",
        "        x = [encoder_train_english, decoder_train_english],\n",
        "        y = decoder_train_indic,\n",
        "        validation_data = ([encoder_val_english, decoder_val_english], decoder_val_indic),\n",
        "        batch_size = self.batch_size,\n",
        "        epochs = self.num_epochs,\n",
        "        # callbacks = [WandbCallback()]\n",
        "        )  \n",
        "\n",
        "    def inference_setup(self):\n",
        "    \n",
        "        encoder_inputs = self.model.input[0]\n",
        "\n",
        "        enc_embed_layer = self.model.get_layer('encoder_embedding')\n",
        "\n",
        "        encoder_outputs = enc_embed_layer(encoder_inputs)\n",
        "\n",
        "        encoder_states = []\n",
        "\n",
        "        if self.cell == 'rnn':\n",
        "            for i in range(self.num_enc_layers):\n",
        "                encoder_outputs, state_h = self.enc_layers[i](encoder_outputs)\n",
        "                encoder_states += [state_h] \n",
        "        elif self.cell == 'lstm':\n",
        "            for i in range(self.num_enc_layers):\n",
        "                encoder_outputs, state_h, state_c = self.enc_layers[i](encoder_outputs)\n",
        "                encoder_states += [state_h, state_c]   \n",
        "        elif self.cell == 'gru':\n",
        "            for i in range(self.num_enc_layers):\n",
        "                encoder_outputs, state_h = self.enc_layers[i](encoder_outputs)\n",
        "                encoder_states += [state_h] \n",
        "\n",
        "        self.encoder_model = keras.Model(encoder_inputs, encoder_states + [encoder_outputs])\n",
        "\n",
        "\n",
        "        decoder_inputs = self.model.input[1]    \n",
        "        dec_embed_layer = self.model.get_layer('decoder_embedding')\n",
        "        decoder_outputs = dec_embed_layer(decoder_inputs)\n",
        "\n",
        "        dec_states = []\n",
        "        dec_initial_states = []\n",
        "        \n",
        "        if self.cell == 'lstm' :\n",
        "            j=0\n",
        "            for i in range(self.num_dec_layers):\n",
        "                dec_initial_states += [Input(shape=(self.num_hidden_layers, )) , Input(shape=(self.num_hidden_layers, ))]\n",
        "                decoder_outputs, state_h, state_c = self.dec_layers[i](decoder_outputs, initial_state=dec_initial_states[i+j:i+j+2])\n",
        "                dec_states += [state_h , state_c]\n",
        "                j += 1\n",
        "\n",
        "        else:\n",
        "            for i in range(self.num_dec_layers):\n",
        "                dec_initial_states += [Input(shape=(self.num_hidden_layers,))]\n",
        "                decoder_outputs, state_h = self.dec_layers[i](decoder_outputs, initial_state = dec_initial_states[i])\n",
        "                dec_states += [state_h]\n",
        "\n",
        "        attention_layer = self.model.get_layer('attention_layer')\n",
        "\n",
        "        attention_input = Input(shape=(self.max_seq_len_english_encoder,self.num_hidden_layers))   \n",
        "\n",
        "        context_vector, alphas = attention_layer([decoder_outputs, attention_input], return_attention_scores=True)\n",
        "    \n",
        "        concat_layer = self.model.get_layer('concatenate_layer')\n",
        "\n",
        "        decoder_outputs = concat_layer([decoder_outputs, context_vector])\n",
        "\n",
        "\n",
        "        # Dense layer\n",
        "        decoder_dense = self.model.get_layer('dense_layer')\n",
        "\n",
        "        decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "        # Decoder model\n",
        "        self.decoder_model = keras.Model(\n",
        "            [decoder_inputs] + dec_initial_states + [attention_input], [decoder_outputs] + dec_states + [alphas])\n",
        "\n",
        "    def decode_sequence(self, input_seq):\n",
        "        self.inference_setup()\n",
        "        enc_states = self.encoder_model.predict(input_seq)\n",
        "        attention_input = enc_states[-1]\n",
        "\n",
        "        enc_states = enc_states[:-1]\n",
        "        \n",
        "        target_seq = np.zeros((1, 1)) \n",
        "        target_seq[0, 0] = self.indic_char_to_idx[\"\\t\"]\n",
        "        \n",
        "        attention_weights = []\n",
        "        stop_condition = False\n",
        "        decoded_sentence = \"\"\n",
        "        while not stop_condition:\n",
        "            output_tokens = self.decoder_model.predict([target_seq] + enc_states + [attention_input])\n",
        "            sampled_token_index = np.argmax(output_tokens[0][0, -1, :])\n",
        "            sampled_char = self.indic_idx_to_char[sampled_token_index]\n",
        "            decoded_sentence += sampled_char\n",
        "\n",
        "            if sampled_char == \"\\n\" or len(decoded_sentence) > self.max_seq_len_indic_decoder:\n",
        "                stop_condition = True\n",
        "\n",
        "            target_seq = np.zeros((1, 1))\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "            enc_states = output_tokens[1:-1]\n",
        "            attention_weights.append(output_tokens[-1][0][0])\n",
        "            \n",
        "        return decoded_sentence, attention_weights\n"
      ],
      "metadata": {
        "id": "gDJf5wFyeY-H"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sweep = api.sweep(\"cs6910_a2/CS6910_A3/7hfn1ujr\")\n",
        "runs = sorted(sweep.runs, key=lambda run: run.summary.get(\"val_accuracy\", 0), reverse=True)\n",
        "runs[0].file(\"model-best.h5\").download(replace=True)\n",
        "print(\"Best model saved to model-best.h5\")\n",
        "image_size = [224,224]\n",
        "input_shape= image_size\n",
        "input_shape.append(3)\n",
        "model = keras.models.load_model('./model-best.h5')\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2C2gYznVeBd",
        "outputId": "b8b57be1-5ad7-469d-8d30-1404c8517067"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model saved to model-best.h5\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " decoder_input (InputLayer)     [(None, 21)]         0           []                               \n",
            "                                                                                                  \n",
            " encoder_embedding (Embedding)  (None, None, 128)    3456        ['encoder_input[0][0]']          \n",
            "                                                                                                  \n",
            " decoder_embedding (Embedding)  (None, 21, 128)      8448        ['decoder_input[0][0]']          \n",
            "                                                                                                  \n",
            " gru (GRU)                      [(None, None, 512),  986112      ['encoder_embedding[0][0]']      \n",
            "                                 (None, 512)]                                                     \n",
            "                                                                                                  \n",
            " gru_1 (GRU)                    [(None, 21, 512),    986112      ['decoder_embedding[0][0]',      \n",
            "                                 (None, 512)]                     'gru[0][1]']                    \n",
            "                                                                                                  \n",
            " attention_layer (AdditiveAtten  ((None, 21, 512),   512         ['gru_1[0][0]',                  \n",
            " tion)                           (None, 21, None))                'gru[0][0]']                    \n",
            "                                                                                                  \n",
            " concatenate_layer (Concatenate  (None, 21, 1024)    0           ['gru_1[0][0]',                  \n",
            " )                                                                'attention_layer[0][0]']        \n",
            "                                                                                                  \n",
            " dense_layer (Dense)            (None, 21, 66)       67650       ['concatenate_layer[0][0]']      \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,052,290\n",
            "Trainable params: 2,052,290\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "c65eqMf_bqmO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}